import requests
from bs4 import BeautifulSoup
import datetime
import os
import json
import hashlib
import time
import random

# --- é«˜çº§é…ç½®åŒº ---
KEYWORDS = ["éŸ¶å…³å¸‚å¦‡å¹¼ä¿å¥é™¢", "éŸ¶å…³å¦‡å¹¼"]
# å®šä¹‰è¦ä¸“é—¨â€œå®šç‚¹çˆ†ç ´â€çš„ç¤¾äº¤å¹³å°åŸŸå
TARGET_SITES = [
   # "", # ç©ºå­—ç¬¦ä¸²ä»£è¡¨å…¨ç½‘æ–°é—»æœç´¢
    "site:weibo.cn", # å¾®åš (ä½¿ç”¨æ‰‹æœºç‰ˆåŸŸåæ”¶å½•æ›´å¿«)
    "site:zhihu.com", # çŸ¥ä¹
    "site:xiaohongshu.com", # å°çº¢ä¹¦
    "site:toutiao.com" # ä»Šæ—¥å¤´æ¡
]
PUSH_TOKEN = os.environ.get("PUSH_TOKEN")
HISTORY_FILE = "history.json"

# è´Ÿé¢æ•æ„Ÿè¯åº“
NEGATIVE_WORDS = ["æŠ•è¯‰", "é¿é›·", "æ€åº¦å·®", "åŒ»ç–—äº‹æ•…", "æ­»", "åƒåœ¾", "å‘", "æ— è¯­", "æ›å…‰", "åµæ¶"]

def get_headers():
    """éšæœºUser-Agentï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨ï¼Œé˜²æ­¢è¢«åçˆ¬"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0'
    ]
    return {'User-Agent': random.choice(user_agents)}

def load_history():
    """è¯»å–å†å²è®°å½•ï¼Œé˜²æ­¢é‡å¤"""
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            try:
                return set(json.load(f))
            except:
                return set()
    return set()

def save_history(history_set):
    """ä¿å­˜å†å²è®°å½•ï¼Œä¿ç•™æœ€è¿‘1000æ¡"""
    # è½¬ä¸ºlistå¹¶åªä¿ç•™æœ€å1000ä¸ªhashï¼Œé˜²æ­¢æ–‡ä»¶æ— é™è†¨èƒ€
    limited_history = list(history_set)[-1000:]
    with open(HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(limited_history, f)

def check_sentiment(text):
    """æ£€æŸ¥æ˜¯å¦åŒ…å«è´Ÿé¢è¯æ±‡"""
    for word in NEGATIVE_WORDS:
        if word in text:
            return True
    return False

def search_bing(keyword, site=""):
    """
    ä½¿ç”¨ Bing æœç´¢ã€‚
    siteå‚æ•°ç”¨äºæŒ‡å®šæœç´¢ç‰¹å®šç½‘ç«™ï¼Œå¦‚ 'site:weibo.cn'
    """
    results = []
    query = f"{keyword} {site}".strip()
    url = f"https://www.bing.com/search?q={query}&sort=date" 
    
    try:
        resp = requests.get(url, headers=get_headers(), timeout=15)
        soup = BeautifulSoup(resp.text, 'lxml')
        
        # é’ˆå¯¹ Bing çš„æ ‡å‡†æœç´¢ç»“æœç»“æ„ (b_algo)
        for item in soup.find_all('li', class_='b_algo'):
            title_tag = item.find('h2')
            if not title_tag: continue
            
            link_tag = title_tag.find('a')
            if not link_tag: continue # ä¿®å¤ç‚¹ï¼šå…ˆæ£€æŸ¥æœ‰æ²¡æœ‰link_tag

            link_link = link_tag.get('href') # ä¿®å¤ç‚¹ï¼šæ‹†åˆ†æˆä¸¤è¡Œå†™ï¼Œé¿å…è¯­æ³•é”™è¯¯
            if not link_link: continue
            
            title = link_tag.text
            # å°è¯•è·å–æ‘˜è¦
            snippet = item.find('p').text if item.find('p') else ""
            if not snippet:
                # å¤‡ç”¨æ‘˜è¦è·å–æ–¹å¼
                caption = item.find('div', class_='b_caption')
                snippet = caption.text if caption else "æ— æ‘˜è¦"

            results.append({
                "title": title,
                "link": link_link,
                "snippet": snippet,
                "source": site if site else "å…¨ç½‘æ–°é—»"
            })
    except Exception as e:
        print(f"æœç´¢ [{query}] æ—¶å‡ºé”™: {e}")
    
    return results

def send_push(content_list, has_risk):
    """å‘é€å¾®ä¿¡æ¨é€"""
    if not content_list: return

    # æ ‡é¢˜åŠ¨æ€å˜åŒ–
    emoji = "âš ï¸" if has_risk else "ğŸ“¢"
    title = f"{emoji} {datetime.date.today()} èˆ†æƒ…æ—¥æŠ¥ ({len(content_list)}æ¡)"
    
    content = "#### ç›‘æ§æ¦‚è§ˆ\n"
    content += f"ç›‘æ§è¯ï¼š{', '.join(KEYWORDS)}\n"
    content += f"è¦†ç›–æºï¼šå¾®åšã€çŸ¥ä¹ã€å¤´æ¡ã€å…¨ç½‘\n\n"
    content += "------------------\n\n"
    content += "\n\n".join(content_list)
    
    url = "http://www.pushplus.plus/send"
    data = {
        "token": PUSH_TOKEN,
        "title": title,
        "content": content,
        "template": "markdown" 
    }
    requests.post(url, json=data)

def main():
    history = load_history()
    new_entries = []
    has_risk = False
    
    print(">>> å¼€å§‹å…¨ç½‘æ‰«æ...")
    
    # åŒé‡å¾ªç¯ï¼šå…³é”®è¯ x ç›®æ ‡ç«™ç‚¹
    for keyword in KEYWORDS:
        for site in TARGET_SITES:
            print(f"æ­£åœ¨æœç´¢: {keyword} @ {site if site else 'å…¨ç½‘'}")
            results = search_bing(keyword, site)
            
            for item in results:
                # ç”Ÿæˆå”¯ä¸€æŒ‡çº¹ (MD5) ç”¨äºå»é‡
                unique_str = item['link']
                uid = hashlib.md5(unique_str.encode()).hexdigest()
                
                if uid in history:
                    continue # è·³è¿‡å·²æ¨é€è¿‡çš„
                
                # å‘½ä¸­æ–°å†…å®¹
                history.add(uid)
                is_negative = check_sentiment(item['title'] + item['snippet'])
                if is_negative: has_risk = True
                
                # æ ¼å¼åŒ–è¾“å‡º
                risk_tag = "**[âš ï¸é«˜å±]** " if is_negative else ""
                entry = f"{risk_tag}**{item['title']}**\n" \
                        f"> æ¥æºï¼š{item['source']}\n" \
                        f"> æ‘˜è¦ï¼š{item['snippet'][:100]}...\n" \
                        f"> [ç‚¹å‡»æŸ¥çœ‹åŸæ–‡]({item['link']})"
                new_entries.append(entry)
            
            # ç¤¼è²Œæ€§å»¶æ—¶ï¼Œé˜²æ­¢è¯·æ±‚è¿‡å¿«è¢«å°
            time.sleep(2)

    if new_entries:
        print(f"å‘ç° {len(new_entries)} æ¡æ–°å†…å®¹ï¼Œæ­£åœ¨æ¨é€...")
        send_push(new_entries, has_risk)
        save_history(history)
        print("å†å²è®°å½•å·²æ›´æ–°ã€‚")
    else:
        print("ä»Šæ—¥æ— æ–°å†…å®¹ã€‚")

if __name__ == "__main__":
    main()
